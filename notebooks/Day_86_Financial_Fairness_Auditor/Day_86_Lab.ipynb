{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Day 86: Financial Fairness Auditor\n",
                "\n",
                "In financial services, 'Safety' includes fairness. Algorithmic bias in loan approvals, credit scoring, or insurance pricing can lead to systemic discrimination against protected groups. In many jurisdictions, this is not just an ethical failure, but a legal one.\n",
                "\n",
                "In this lab, we implement a **Financial Fairness Auditor** to:\n",
                "1. **Calculate Statistical Parity**: Measuring the approval rates across different demographic groups.\n",
                "2. **Check for Disparate Impact**: Using the 'Four-Fifths Rule' (80% Rule) to determine if a minority group is being significantly disadvantaged.\n",
                "3. **Audit Loan Models**: Running audits on synthetic decision datasets to identify models that require retraining or structural adjustment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "\n",
                "# Add root directory to sys.path\n",
                "sys.path.append(os.path.abspath('../../'))\n",
                "\n",
                "from src.ethics.financial_auditor import FinancialFairnessAuditor"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Scenario A: A Biased Loan Model\n",
                "\n",
                "We analyze a model that appears to favor one gender over another."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "auditor = FinancialFairnessAuditor()\n",
                "\n",
                "biased_data = [\n",
                "    {\"gender\": \"Male\", \"approved\": 1}] * 80 + \\\n",
                "    [{\"gender\": \"Male\", \"approved\": 0}] * 20 + \\\n",
                "    [{\"gender\": \"Female\", \"approved\": 1}] * 40 + \\\n",
                "    [{\"gender\": \"Female\", \"approved\": 0}] * 60\n",
                "\n",
                "audit = auditor.audit_loan_model(biased_data)\n",
                "print(f\"Group Rates: {audit['group_rates']}\")\n",
                "print(f\"Impact Ratio: {audit['impact_analysis']['ratio']}\")\n",
                "print(f\"Verdict: {audit['impact_analysis']['recommendation']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Scenario B: A Fair (Compliant) Model\n",
                "\n",
                "Even if rates aren't identical, they may still be fair according to regulatory standards."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fair_data = [\n",
                "    {\"gender\": \"GroupA\", \"approved\": 1}] * 70 + \\\n",
                "    [{\"gender\": \"GroupA\", \"approved\": 0}] * 30 + \\\n",
                "    [{\"gender\": \"GroupB\", \"approved\": 1}] * 65 + \\\n",
                "    [{\"gender\": \"GroupB\", \"approved\": 0}] * 35\n",
                "\n",
                "audit = auditor.audit_loan_model(fair_data)\n",
                "print(f\"Group Rates: {audit['group_rates']}\")\n",
                "print(f\"Impact Ratio: {audit['impact_analysis']['ratio']}\")\n",
                "print(f\"Verdict: {audit['impact_analysis']['recommendation']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Beyond Simple Audits\n",
                "\n",
                "Simply measuring bias isn't enough. In production, safety engineers use **adversarial debiasing** (Day 67) or **re-weighting** techniques to fix these issues during the training phase. This auditor serves as the 'verification gate' that decides if a model is safe to deploy."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}