{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Day 24: Safety Case Builder\n",
                "\n",
                "In this lab, we will build a **Safety Case** for a hypothetical LLM deployment. A Safety Case is a structured argument, supported by evidence, that explains why a system is safe to operate in a specific context.\n",
                "\n",
                "We will use the `SafetyCase`, `Claim`, `Argument`, and `Evidence` classes we implemented."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "\n",
                "# Add root directory to sys.path\n",
                "sys.path.append(os.path.abspath('../../'))\n",
                "\n",
                "from src.assurance.safety_case import SafetyCase, Claim, Argument, Evidence"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Define the Top-Level Safety Claim\n",
                "\n",
                "Our top-level claim is usually something like: \"The model is safe for public deployment as a customer service chatbot.\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "safety_case = SafetyCase(title=\"Customer Service Chatbot deployment\", version=\"1.0\")\n",
                "\n",
                "top_claim = Claim(\n",
                "    id=\"C1\",\n",
                "    description=\"The model is safe for use in a customer service chatbot role.\"\n",
                ")\n",
                "safety_case.add_claim(top_claim)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Break Down into Sub-Claims\n",
                "\n",
                "To prove C1, we need to prove several sub-claims:\n",
                "- **C1.1**: The model is robust to prompt injection attacks.\n",
                "- **C1.2**: The model refuses to generate harmful content (hate speech, PII).\n",
                "- **C1.3**: The model does not hallucinate critical facts."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "claim_1_1 = Claim(id=\"C1.1\", description=\"The model is robust to prompt injection attacks.\")\n",
                "claim_1_2 = Claim(id=\"C1.2\", description=\"The model refuses to generate harmful content.\")\n",
                "\n",
                "# Link sub-claims (conceptually, in a full tree we would automate this linking,\n",
                "# here we just add them to the case)\n",
                "safety_case.add_claim(claim_1_1)\n",
                "safety_case.add_claim(claim_1_2)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Add Evidence\n",
                "\n",
                "We'll simulate evidence from our previous days' tools (Red Teaming, Refusal Calibration)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evidence from Day 16 (Automated Red Teaming)\n",
                "ev_red_team = Evidence(\n",
                "    id=\"E1\",\n",
                "    description=\"Red Teaming Report - 50 iterations\",\n",
                "    data={\"attack_success_rate\": 0.02, \"total_attempts\": 1000}\n",
                ")\n",
                "\n",
                "# Evidence from Day 14 (Refusal Calibration)\n",
                "ev_refusal = Evidence(\n",
                "    id=\"E2\",\n",
                "    description=\"Refusal Calibration Test\",\n",
                "    data={\"false_refusal_rate\": 0.05, \"true_refusal_rate\": 0.98}\n",
                ")\n",
                "\n",
                "safety_case.add_evidence(ev_red_team)\n",
                "safety_case.add_evidence(ev_refusal)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Construct Arguments\n",
                "\n",
                "Now we link the evidence to the claims."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Argument for C1.1 (Robust to attacks)\n",
                "arg_1_1 = Argument(\n",
                "    id=\"A1\",\n",
                "    description=\"Red teaming shows low attack success rate (< 5%).\",\n",
                "    evidence_ids=[\"E1\"]\n",
                ")\n",
                "claim_1_1.arguments.append(arg_1_1)\n",
                "\n",
                "# Argument for C1.2 (Harmful content)\n",
                "arg_1_2 = Argument(\n",
                "    id=\"A2\",\n",
                "    description=\"Calibration test shows high True Refusal Rate (> 95%).\",\n",
                "    evidence_ids=[\"E2\"]\n",
                ")\n",
                "claim_1_2.arguments.append(arg_1_2)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Evaluate and Generate Report\n",
                "\n",
                "We run the evaluation logic to see if our claims are supported."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "safety_case.evaluate()\n",
                "report = safety_case.generate_report()\n",
                "print(report)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}