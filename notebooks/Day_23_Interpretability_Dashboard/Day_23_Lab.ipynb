{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Day 23: Interpretability Dashboard\n",
                "\n",
                "## \ud83d\udd2c Objective\n",
                "Visualize **Attention Weights** to understand how a model routes information. Attention tells us \"When generating token X, which previous tokens was I looking at?\"\n",
                "\n",
                "## \ud83e\udde0 Induction Heads\n",
                "One key circuit in LLMs is the \"Induction Head\", which copies information. E.g., if the text says \"Harry Potter\", encountering \"Harry\" later makes the model attend back to \"Potter\"."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "import numpy as np\n",
                "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../../\")))\n",
                "\n",
                "from src.observability.interpretability import AttentionVisualizer"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 1: Simulate Attention\n",
                "Create a sequence [A, B, A, B]. When generating the 2nd 'B' (index 3), the model should look at the 1st 'B' (index 1)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tokens = [\"Harry\", \"Potter\", \"ate\", \"Harry\", \"Potter\"]\n",
                "viz = AttentionVisualizer(tokens)\n",
                "\n",
                "# Create specific attention matrix for our 'Induction Head'\n",
                "# 5x5 matrix\n",
                "matrix = np.zeros((5, 5))\n",
                "\n",
                "# Standard causal mask (can't see future) - not strictly enforced here for simple viz\n",
                "\n",
                "# Token 4 ('Potter') attends strongly to Token 1 ('Potter')\n",
                "matrix[4, 1] = 0.95\n",
                "matrix[4, 4] = 0.05 # Self attention\n",
                "\n",
                "viz.add_attention(layer=1, head=1, matrix=matrix)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 2: Analyze Focus\n",
                "Check where the last token is looking."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "focus = viz.get_strongest_focus(token_index=4, layer=1, head=1)\n",
                "print(f\"Token '{focus['source_token']}' (idx 4) is focusing on '{focus['focused_token']}' (idx {focus['focused_index']}) with weight {focus['weight']}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}