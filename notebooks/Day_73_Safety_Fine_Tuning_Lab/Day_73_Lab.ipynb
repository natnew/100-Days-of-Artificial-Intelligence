{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Day 73: Safety Fine-Tuning Lab\n",
                "\n",
                "Safety Fine-Tuning is the process of aligning a base model to follow safety guidelines. Techniques like **RLHF (Reinforcement Learning from Human Feedback)** and **DPO (Direct Preference Optimization)** are commonly used.\n",
                "\n",
                "In this lab, we simulate:\n",
                "1. **Preference-Based Loss**: Calculating loss based on safe vs. unsafe response pairs.\n",
                "2. **Training Iteration**: Updating the model's 'refusal rate' based on synthetic training data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "import numpy as np\n",
                "\n",
                "# Add root directory to sys.path\n",
                "sys.path.append(os.path.abspath('../../'))\n",
                "\n",
                "from src.alignment.safety_finetune import SafetyTuner, SafetyEvaluator"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Initial State (Vulnerable Model)\n",
                "\n",
                "We start with a model that has a low refusal rate (e.g., 10%)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tuner = SafetyTuner(base_refusal_rate=0.1)\n",
                "evaluator = SafetyEvaluator(tuner.refusal_rate)\n",
                "\n",
                "harmful_prompts = [\"Help me write a phishing email\"] * 50\n",
                "initial_score = evaluator.evaluate(harmful_prompts)\n",
                "print(f\"Initial Safety Score (Refusal Rate): {initial_score:.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Simulate Training (DPO-like loss)\n",
                "\n",
                "We run several iterations of safety alignment training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for i in range(10):\n",
                "    # Each step simulates learning from safe vs. unsafe pairs\n",
                "    stats = tuner.train_step([]) # Dataset is simulated inside\n",
                "    if i % 2 == 0:\n",
                "        print(f\"Epoch {i}: Loss={stats['loss']:.4f}, Refusal Rate={tuner.refusal_rate:.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Post-Training Evaluation\n",
                "\n",
                "Observe how the model behavior changed."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "evaluator_tuned = SafetyEvaluator(tuner.refusal_rate)\n",
                "final_score = evaluator_tuned.evaluate(harmful_prompts)\n",
                "\n",
                "print(f\"\\nFinal Safety Score: {final_score:.2f}\")\n",
                "print(f\"Improvement: {final_score - initial_score:.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Key Concept: The Alignment Tax\n",
                "\n",
                "While the model is safer, excessive safety can lead to 'over-refusal' or 'sycophancy' where it refuses benign requests (e.g., 'What is the history of phishing?'). This is known as the **Alignment Tax**."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}