{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Day 70: Membership Inference Defense\n",
                "\n",
                "Membership Inference Attacks (MIA) allow an adversary to determine if a specific data point was used to train a target model. \n",
                "\n",
                "In this lab, we implement:\n",
                "1. **MIAttacker**: A threshold-based attack that exploits model overfitting.\n",
                "2. **MIDefender**: An inference-time defense using **Output Perturbation** (Laplacian noise)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "import numpy as np\n",
                "\n",
                "# Add root directory to sys.path\n",
                "sys.path.append(os.path.abspath('../../'))\n",
                "\n",
                "from src.privacy.membership_inference import MIAttacker, MIDefender\n",
                "from sklearn.ensemble import RandomForestClassifier"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Simulate Overfitting (Vulnerability)\n",
                "\n",
                "We use a `MockModel` that is highly confident on training data and less so on test data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class VulnerableModel:\n",
                "    def predict_proba(self, X):\n",
                "        # Mock: High confidence (0.99) for members (X[0] < 0.5), Low (0.6) for non-members\n",
                "        return np.array([[0.01, 0.99] if x[0] < 0.5 else [0.4, 0.6] for x in X])\n",
                "\n",
                "model = VulnerableModel()\n",
                "members_X = np.array([[0.1, 0.1] for _ in range(50)])\n",
                "non_members_X = np.array([[0.9, 0.9] for _ in range(50)])\n",
                "y = np.ones(50, dtype=int)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Execute Attack (No Defense)\n",
                "\n",
                "The attacker predicts 'Member' if confidence is > 0.8."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "attacker = MIAttacker()\n",
                "member_preds = attacker.attack_threshold_based(model, members_X, y, threshold=0.8)\n",
                "non_member_preds = attacker.attack_threshold_based(model, non_members_X, y, threshold=0.8)\n",
                "\n",
                "print(\"Undefended Results:\", attacker.evaluate_attack(member_preds, non_member_preds))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Apply Defense (Output Perturbation)\n",
                "\n",
                "We add noise to the output probabilities to mask the membership signal."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "defender = MIDefender(noise_scale=0.25)\n",
                "\n",
                "class DefendedModel:\n",
                "    def predict_proba(self, X):\n",
                "        probs = model.predict_proba(X)\n",
                "        return defender.perturb_outputs(probs)\n",
                "\n",
                "defended_model = DefendedModel()\n",
                "member_preds_def = attacker.attack_threshold_based(defended_model, members_X, y, threshold=0.8)\n",
                "non_member_preds_def = attacker.attack_threshold_based(defended_model, non_members_X, y, threshold=0.8)\n",
                "\n",
                "print(\"Defended Results:\", attacker.evaluate_attack(member_preds_def, non_member_preds_def))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}