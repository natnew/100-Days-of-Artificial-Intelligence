{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Day 27: Counterfactual Generator\n",
                "\n",
                "In this lab, we will use the `CounterfactualGenerator` to test for **Individual Fairness**.\n",
                "Individual Fairness asserts that similar individuals should be treated similarly. By flipping sensitive attributes (like gender) while keeping the rest of the context identical, we can probe if the model is biased."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "\n",
                "# Add root directory to sys.path\n",
                "sys.path.append(os.path.abspath('../../'))\n",
                "\n",
                "from src.fairness.counterfactual.py import CounterfactualGenerator # Typo fix: removed .py in actual code\n",
                "from src.fairness.counterfactual import CounterfactualGenerator"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Generate Counterfactuals\n",
                "\n",
                "Let's take a set of prompts involving people and generate their gender-swapped counterparts."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "generator = CounterfactualGenerator()\n",
                "\n",
                "prompts = [\n",
                "    \"The doctor said he is busy.\",\n",
                "    \"She is a brilliant engineer.\",\n",
                "    \"The King ruled the kingdom wisely.\",\n",
                "    \"His mother was very proud of him.\"\n",
                "]\n",
                "\n",
                "pairs = []\n",
                "for p in prompts:\n",
                "    cf_list = generator.generate_gender_counterfactuals(p)\n",
                "    if cf_list:\n",
                "        pairs.append((p, cf_list[0]))\n",
                "    else:\n",
                "        pairs.append((p, None))\n",
                "\n",
                "for original, cf in pairs:\n",
                "    print(f\"Original: {original}\")\n",
                "    print(f\"Counterfactual: {cf}\")\n",
                "    print(\"---\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Simulate a Biased Model\n",
                "\n",
                "We will create a mock function that simulates a model with gender bias. \n",
                "For example, it might associate 'he' with 'career' and 'she' with 'family'."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def biased_sentiment_model(text):\n",
                "    # Mock bias: assigns higher sentiment to male pronouns in professional contexts\n",
                "    score = 0.5\n",
                "    text = text.lower()\n",
                "    \n",
                "    if \"engineer\" in text or \"doctor\" in text:\n",
                "        if \"he\" in text or \"man\" in text:\n",
                "            score += 0.3\n",
                "        elif \"she\" in text or \"woman\" in text:\n",
                "            score -= 0.1\n",
                "            \n",
                "    return score\n",
                "\n",
                "# Test the model\n",
                "print(f\"Score (He is a doctor): {biased_sentiment_model('He is a doctor')}\")\n",
                "print(f\"Score (She is a doctor): {biased_sentiment_model('She is a doctor')}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Evaluate Consistency\n",
                "\n",
                "Now we check if the model treats the counterfactuals consistently."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Evaluating Consistency...\\n\")\n",
                "\n",
                "for original, cf in pairs:\n",
                "    if not cf:\n",
                "        continue\n",
                "        \n",
                "    score_orig = biased_sentiment_model(original)\n",
                "    score_cf = biased_sentiment_model(cf)\n",
                "    \n",
                "    diff = abs(score_orig - score_cf)\n",
                "    is_consistent = diff < 0.01 # Tolerance\n",
                "    \n",
                "    print(f\"Original: '{original}' -> Score: {score_orig}\")\n",
                "    print(f\"CounterF: '{cf}' -> Score: {score_cf}\")\n",
                "    print(f\"Consistent? {is_consistent} (Diff: {diff:.2f})\")\n",
                "    print(\"---\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}