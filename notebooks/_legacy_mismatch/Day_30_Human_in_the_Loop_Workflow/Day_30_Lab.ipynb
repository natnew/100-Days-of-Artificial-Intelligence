{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Day 30: Human-in-the-Loop Workflow\n",
                "\n",
                "In this lab, we will simulate a **Content Moderation** pipeline.\n",
                "If the Model's confidence is low, the request will be escalated to a queue for human review (`HITLManager`)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "import time\n",
                "\n",
                "# Add root directory to sys.path\n",
                "sys.path.append(os.path.abspath('../../'))\n",
                "\n",
                "from src.governance.hitl.py import HITLManager # Typo correction: logic should be from src.governance.hitl\n",
                "from src.governance.hitl import HITLManager"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Simulate Content and Confidence\n",
                "\n",
                "We pretend we have processed 5 posts. Some are clearly safe/unsafe (high confidence), others are ambiguous (low confidence)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# (Post Content, Model Prediction, Confidence Score)\n",
                "pipeline_outputs = [\n",
                "    (\"This is a nice kitty.\", \"SafePrediction\", 0.99),\n",
                "    (\"I hate everyone.\", \"UnsafePrediction\", 0.95),\n",
                "    (\"I don't like Mondays.\", \"UnsafePrediction\", 0.55), # Low confidence, sounds negative but maybe safe?\n",
                "    (\"He is annoying.\", \"UnsafePrediction\", 0.60),\n",
                "    (\"Hello world.\", \"SafePrediction\", 0.98)\n",
                "]\n",
                "\n",
                "# Manager configured to escalate if confidence < 0.8\n",
                "manager = HITLManager(escalation_threshold=0.8)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Process Items\n",
                "\n",
                "Route them through the manager."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "final_decisions = []\n",
                "\n",
                "for content, pred, score in pipeline_outputs:\n",
                "    review_id = manager.analyze_and_route(content, score)\n",
                "    \n",
                "    if review_id:\n",
                "        print(f\"[ESCALATED] '{content}' (Score: {score}) -> Review ID: {review_id}\")\n",
                "    else:\n",
                "        print(f\"[AUTO-APPROVED] '{content}' (Score: {score}) -> prediction accepted.\")\n",
                "        final_decisions.append((content, pred, \"Auto\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Human Review\n",
                "\n",
                "The manager now has pending requests. In a real app, these would appear on a dashboard."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pending = manager.get_pending_requests()\n",
                "print(f\"\\nPending Reviews: {len(pending)}\")\n",
                "\n",
                "for req in pending:\n",
                "    print(f\"Reviewing: {req.content} (Reason: {req.reason})\")\n",
                "    # Simulate human feedback\n",
                "    # Human decides: \"I don't like Mondays\" is Safe, \"He is annoying\" is Unsafe\n",
                "    if \"Mondays\" in req.content:\n",
                "        decision = \"Approved (Overridden to Safe)\"\n",
                "        manager.submit_feedback(req.id, \"Modify\", \"Marked as Safe\")\n",
                "    else:\n",
                "        decision = \"Confirmed Unsafe\"\n",
                "        manager.submit_feedback(req.id, \"Approve\", \"Confirmed model prediction\")\n",
                "    \n",
                "    final_decisions.append((req.content, decision, \"Human\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Final Logs\n",
                "\n",
                "Show the final audit trail."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "df = pd.DataFrame(final_decisions, columns=[\"Content\", \"Outcome\", \"Actor\"])\n",
                "print(df)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}