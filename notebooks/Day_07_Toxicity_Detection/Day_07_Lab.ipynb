{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Day 07: Toxicity Detection\n",
                "\n",
                "## \u2622\ufe0f Objective\n",
                "Implement a safety filter that detects and redacts toxic language from AI outputs.\n",
                "\n",
                "## \ud83d\udee1\ufe0f Concept\n",
                "**Content Moderation**: Essential for preventing AI from generating hate speech, insults, or dangerous content."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../../\")))\n",
                "\n",
                "from src.evaluators.toxicity import ToxicityClassifier, ToxicityFilter"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 1: Score Inputs\n",
                "We check how \"toxic\" specific phrases are using our keyword-based model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "classifier = ToxicityClassifier()\n",
                "\n",
                "examples = [\n",
                "    \"Hello friend!\",\n",
                "    \"You are a stupid idiot.\",\n",
                "    \"I hate this.\"\n",
                "]\n",
                "\n",
                "for ex in examples:\n",
                "    print(f\"Input: {ex}\")\n",
                "    print(classifier.predict(ex))\n",
                "    print(\"-\" * 20)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 2: Auto-Redaction\n",
                "Apply a filter to clean up the output."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "filtr = ToxicityFilter()\n",
                "\n",
                "toxic_input = \"Shut up you idiot.\"\n",
                "cleaned = filtr.redact(toxic_input)\n",
                "\n",
                "print(f\"Original: {toxic_input}\")\n",
                "print(f\"Cleaned:  {cleaned}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}