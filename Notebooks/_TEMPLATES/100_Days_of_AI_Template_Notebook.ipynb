{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ],
      "metadata": {
        "id": "ltotQaJtYJnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **\"100 Days of Artificial Intelligence\"** project is an initiative to explore various aspects of artificial intelligence (AI) over 100 days. The project aims to comprehensively understand AI techniques, applications, and their real-world implications. Each project day focuses on a specific topic or concept related to AI, ranging from image classification and natural language processing to deep learning and reinforcement learning."
      ],
      "metadata": {
        "id": "EoyNJfMaZjRN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goals of the project are:\n",
        "* Learning Journey: To embark on a structured and educational journey through AI's diverse landscape, covering fundamental concepts and advanced techniques.\n",
        "* Skill Development: To enhance skills in AI programming, data analysis, and model building by working on practical projects and exercises.\n",
        "* Knowledge Sharing: To foster a community of AI enthusiasts and practitioners, sharing insights, resources, and experiences related to AI.\n",
        "* Holistic Understanding: To gain a holistic understanding of AI by exploring various subfields, including computer vision, natural language processing, machine learning, and more.\n",
        "* Practical Applications: To develop hands-on experience by implementing AI projects that can be applied in real-world scenarios across different industries."
      ],
      "metadata": {
        "id": "DYa5NWb3ZkCE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*No specific background in AI is required to participate in the project, as it caters to individuals at different levels of expertise, including beginners and experienced practitioners. The project encourages active learning, collaboration, and engagement with the AI community through discussions, code sharing, and knowledge exchange.*"
      ],
      "metadata": {
        "id": "Pb3pD9ciZ26M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Repo\n",
        "Please refer to the project repository for more details and updates:\n",
        "[Project Repo](https://github.com/natnew/100-Days-of-Artificial-Intelligence/tree/main)\n",
        "\n",
        "## Usage and Attribution\n",
        "If you find this material useful, we kindly request that you give appropriate credit and attribute the project repository in your work. You can include a reference to the project repo in your code comments, documentation, or acknowledgments section, like this:\n",
        "\n",
        "'''\n",
        "Code adapted from the \"100-Days-of-Artificial-Intelligence\" project repository by [100-Days-of-Artificial-Intelligence](https://github.com/natnew/100-Days-of-Artificial-Intelligence/tree/main)\n",
        "'''\n",
        "\n",
        "Thank you for your understanding and support!\n"
      ],
      "metadata": {
        "id": "PSFQbKQJeAUm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Business Understanding\n"
      ],
      "metadata": {
        "id": "Q4DrK_7mXLVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we provide an overview of the business problem that the project aims to address and the goals it aims to achieve. It is important to understand the context and motivation behind the project, as well as the potential impact it can have on the business or industry. This section helps to establish the relevance and importance of the project in the larger business context. It also sets the stage for defining the project's s**cope and success criteria.**\n",
        "\n",
        "#### **Industry**\n",
        "[PLACEHOLDER] <br>\n",
        ".... <br>\n",
        "....\n",
        "\n",
        "\n",
        "#### **Hypothesis**\n",
        "[PLACEHOLDER] <br>\n",
        ".... <br>\n",
        "....\n",
        "\n",
        "#### **Goal/Success Criteria**\n",
        "[PLACEHOLDER] <br>\n",
        ".... <br>\n",
        "...."
      ],
      "metadata": {
        "id": "MzZAoKaPXOps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Libraries"
      ],
      "metadata": {
        "id": "x0pf7P1uYL9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy pandas tensorflow torch openai matplotlib\n",
        "# Install Locally - Google Collab"
      ],
      "metadata": {
        "id": "y6gKQmmEYOAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import openai\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Add additional libraries if needed."
      ],
      "metadata": {
        "id": "Fox1BKGMaVy3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "sY1esyfyYOeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to the CSV file\n",
        "csv_path = 'data/data.csv'\n",
        "\n",
        "# Load the CSV file into a pandas DataFrame\n",
        "data = pd.read_csv(csv_path)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "BhRHl2I2YQNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "ME_i6RRIYQqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary Stats: Assuming 'data' is your loaded DataFrame\n",
        "summary_stats = data.describe()\n",
        "\n",
        "# Print the summary statistics\n",
        "print(summary_stats)"
      ],
      "metadata": {
        "id": "h2ipcikXYStF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Histogram: Assuming 'data' is your DataFrame and 'column' is the column you want to plot\n",
        "data['column'].plot(kind='hist')\n",
        "plt.xlabel('X-axis label')\n",
        "plt.ylabel('Y-axis label')\n",
        "plt.title('Histogram')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lNS-HetwbPyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatter plot:Assuming 'data' is your DataFrame and 'x' and 'y' are the columns you want to plot\n",
        "plt.scatter(data['x'], data['y'])\n",
        "plt.xlabel('X-axis label')\n",
        "plt.ylabel('Y-axis label')\n",
        "plt.title('Scatter Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hC-BRBKqbsPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bar plot:Assuming 'data' is your DataFrame and 'column' is the column you want to plot\n",
        "data['column'].value_counts().plot(kind='bar')\n",
        "plt.xlabel('X-axis label')\n",
        "plt.ylabel('Y-axis label')\n",
        "plt.title('Bar Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Jc6jvLXbbwcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify missing values - Assuming 'data' is your DataFrame\n",
        "\n",
        "# Identify missing values\n",
        "missing_values = data.isnull().sum()\n",
        "\n",
        "# Print the count of missing values for each column\n",
        "print(missing_values)\n",
        "\n",
        "# Handle missing values by imputing or removing them\n",
        "# Option 1: Impute missing values with a specific value or statistic\n",
        "data_filled = data.fillna(0)  # Fill missing values with 0\n",
        "\n",
        "# Option 2: Remove rows or columns with missing values\n",
        "data_cleaned = data.dropna()  # Remove rows with any missing values\n",
        "\n",
        "# Option 3: Impute missing values with mean, median, or other statistics\n",
        "data_imputed = data.fillna(data.mean())  # Fill missing values with column mean\n",
        "\n",
        "# Print the modified DataFrame to verify changes\n",
        "print(data_filled.head())\n",
        "print(data_cleaned.head())\n",
        "print(data_imputed.head())"
      ],
      "metadata": {
        "id": "Q-yLgNh7b7fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect outliers: Assuming 'data' is your DataFrame\n",
        "\n",
        "# Detect outliers using box plots\n",
        "sns.boxplot(data=data)\n",
        "\n",
        "# Detect outliers using scatter plots\n",
        "sns.scatterplot(x='x_column', y='y_column', data=data)\n",
        "\n",
        "# Detect outliers using statistical methods (z-scores or IQR)\n",
        "\n",
        "# Z-score method\n",
        "z_scores = (data - data.mean()) / data.std()\n",
        "outliers_zscore = data[(z_scores > 3).any(axis=1)]\n",
        "\n",
        "# IQR method\n",
        "Q1 = data.quantile(0.25)\n",
        "Q3 = data.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "outliers_iqr = data[((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "\n",
        "# Decide how to handle outliers based on domain knowledge\n",
        "# Options: remove outliers, replace with a specific value, or apply transformations\n",
        "# For example, let's replace outliers with the column median\n",
        "data_cleaned = data.copy()\n",
        "data_cleaned[(z_scores > 3).any(axis=1)] = data.median()\n",
        "\n",
        "# Print the modified DataFrame to verify changes\n",
        "print(data_cleaned.head())"
      ],
      "metadata": {
        "id": "VA5suW-ScRkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering"
      ],
      "metadata": {
        "id": "O9y00rrTYTH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature engineering by extracting month and year from a date columnAssuming 'data' is your DataFrame\n",
        "\n",
        "# Convert the date column to datetime format if it's not already\n",
        "data['date_column'] = pd.to_datetime(data['date_column'])\n",
        "\n",
        "# Extract month and year from the date column\n",
        "data['month'] = data['date_column'].dt.month\n",
        "data['year'] = data['date_column'].dt.year\n",
        "\n",
        "# Print the modified DataFrame to verify changes\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "nGp9mAvFcqSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature engineering where we create a new feature by combining existing features: Assuming 'data' is your DataFrame\n",
        "\n",
        "# Create a new feature by combining two existing features\n",
        "data['new_feature'] = data['feature1'] + data['feature2']\n",
        "\n",
        "# Print the modified DataFrame to verify changes\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "ImGf-VFtdb-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature engineering where we encode categorical variables using one-hot encoding. Assuming 'data' is your DataFrame\n",
        "\n",
        "# Perform one-hot encoding on the 'category' column\n",
        "encoded_data = pd.get_dummies(data, columns=['category'])\n",
        "\n",
        "# Print the modified DataFrame to verify changes\n",
        "print(encoded_data.head())"
      ],
      "metadata": {
        "id": "x_0GIgyWdivK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature engineering where we create a new feature by applying a mathematical transformation to an existing feature. Assuming 'data' is your DataFrame\n",
        "\n",
        "# Create a new feature by taking the square root of an existing feature\n",
        "data['sqrt_feature'] = data['feature'] ** 0.5\n",
        "\n",
        "# Print the modified DataFrame to verify changes\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "_0NkZgdKecHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation analysis: Assuming 'data' is your DataFrame\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = data.corr()\n",
        "\n",
        "# Create a heatmap of the correlation matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y_1M58Qhc_-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Development"
      ],
      "metadata": {
        "id": "n_t7g0xSYWRU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decision Tree Classifier\n",
        "A Decision Tree Classifier is a supervised machine learning algorithm for classification tasks. It builds a tree-like model where each internal node represents a feature, each branch represents a decision based on that feature, and each leaf node represents a class label. The model predicts by traversing the tree from the root to a leaf node based on the input features. It is popular due to its interpretability and ability to handle categorical and numerical data. Decision trees are used when the relationship between features and the target variable is non-linear or when feature interactions are important.\n",
        "\n",
        "#### For example:\n",
        "A Decision Tree Classifier can be used to predict whether a student will pass or fail an exam based on factors like study hours, attendance, and previous grades. By analyzing these factors, the model can provide a clear decision boundary to classify new students and help identify those who may need additional support to succeed."
      ],
      "metadata": {
        "id": "c0X4oUgMAIvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree Classifier:\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Create a decision tree classifier object\n",
        "model = DecisionTreeClassifier()\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "ZprQAB8TYYj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random Forest Classifier\n",
        "A Random Forest Classifier is an ensemble model that combines multiple decision trees to make predictions. It works by creating a collection of decision trees on different subsets of the data and averaging their predictions to make the final prediction. The model is used for classification tasks and is known for its ability to handle complex datasets, reduce overfitting, and provide robust predictions. It is widely used in various domains, including finance, healthcare, and marketing, where accurate and reliable classification is crucial.\n",
        "\n",
        "\n",
        "#### For example:\n",
        "A Random Forest Classifier can be used to predict whether an email is spam or not based on its content and other features. It is also used in fraud detection systems to identify suspicious transactions by analyzing multiple factors."
      ],
      "metadata": {
        "id": "KLU8wxEdUh_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Classifier:\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Create a random forest classifier object\n",
        "model = RandomForestClassifier()\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "RvKwhynp_wpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Support Vector Machine (SVM) Classifier\n",
        "A Support Vector Machine (SVM) Classifier is a machine learning model used for classification tasks. It separates data points into different classes by creating a decision boundary. It works by finding the best hyperplane that maximally separates the classes in the feature space. SVMs are effective in handling high-dimensional data and can handle both linear and non-linear classification problems using different kernel functions. They are commonly used in various applications such as text classification, image recognition, and bioinformatics due to their ability to handle complex datasets and provide robust classification performance.\n",
        "\n",
        "\n",
        "#### For example\n",
        "Support Vector Machine (SVM) Classifier can be used for credit card fraud detection. By training on labeled data containing fraudulent and non-fraudulent transactions, SVM can learn patterns and detect anomalies in new credit card transactions. SVMs are effective in handling imbalanced datasets and can accurately classify transactions as fraudulent or legitimate."
      ],
      "metadata": {
        "id": "KIeM8WmCVv-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Support Vector Machine (SVM) Classifier:\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Create an SVM classifier object\n",
        "model = SVC()\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "GKt8yXbP_0K_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Deep Learning Model (using Keras with TensorFlow backend):\n",
        "A Deep Learning Model, built using Keras with TensorFlow backend, is a powerful neural network model that learns hierarchical representations of data. It consists of multiple layers of interconnected artificial neurons that can extract complex features from raw input data. Deep learning models excel in tasks such as image recognition, natural language processing, and speech recognition. They are capable of automatically learning from large amounts of data and can handle complex patterns. Deep learning models are used when high accuracy and performance are required in tasks involving large and complex datasets.\n",
        "\n",
        "#### For example\n",
        "A deep learning model can be used to identify objects in images, such as detecting if an image contains a cat or a dog. It can also be used for speech recognition, like converting spoken words into text."
      ],
      "metadata": {
        "id": "qkiRUkb4Wln3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Deep Learning Model (using Keras with TensorFlow backend):\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Create a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add layers to the model\n",
        "model.add(Dense(64, activation='relu', input_shape=(input_dim,)))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "0vAQ1PTO_3SP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Logistic Regression\n",
        "Logistic Regression is a statistical model used for binary classification tasks, where the goal is to predict the probability of an instance belonging to a particular class. It calculates a weighted linear combination of input features and applies a sigmoid function to produce the probability output. The model is trained using a maximum likelihood estimation approach to find the optimal weights that minimize the error between predicted probabilities and actual class labels. Logistic Regression is commonly used due to its simplicity, interpretability, and efficiency in handling large datasets. It is suitable when the relationship between features and the target variable is assumed to be linear.\n",
        "\n",
        "#### For example\n",
        "Logistic Regression can be used in healthcare to predict the likelihood of a patient having a certain disease based on their medical history, demographic information, and other relevant factors. This can help healthcare professionals identify high-risk individuals who may require further diagnostic tests or early intervention, leading to more effective treatment and improved patient outcomes."
      ],
      "metadata": {
        "id": "bLagf0CdlDEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression:\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Create the Logistic Regression model\n",
        "logistic_regression_model = LogisticRegression()\n",
        "\n",
        "# Train the model\n",
        "logistic_regression_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = logistic_regression_model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "01o8rw1kkiNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Gradient Boosting\n",
        "Gradient Boosting is a machine learning ensemble method that combines multiple weak prediction models, typically decision trees, to create a strong predictive model. It works by iteratively training new models that focus on the errors made by previous models, gradually improving the overall prediction accuracy. Gradient Boosting is used for both classification and regression tasks and is popular due to its ability to handle complex data patterns and provide accurate predictions. It is often used in applications such as click-through rate prediction, fraud detection, and personalized recommendation systems.\n",
        "\n",
        "#### For example\n",
        "Gradient Boosting can be used in scenarios where accurate predictions are crucial, such as predicting customer churn, identifying credit card fraud, or determining disease risk based on patient data. It excels in situations where the data is complex and requires a model that can capture intricate patterns and make precise predictions."
      ],
      "metadata": {
        "id": "c_r4G-IKlHPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Boosting:\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# Create the Gradient Boosting model\n",
        "gradient_boosting_model = GradientBoostingClassifier()\n",
        "\n",
        "# Train the model\n",
        "gradient_boosting_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = gradient_boosting_model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "-nZfSQXlknw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Naive Bayes\n",
        "Naive Bayes is a probabilistic classification model that assumes independence among the features. It is commonly used for text classification tasks, such as spam detection and sentiment analysis, as well as in recommendation systems and document categorization. Naive Bayes calculates the probability of each class given the input features using Bayes' theorem. It is efficient, simple to implement, and works well with high-dimensional data. Despite its assumption of feature independence, Naive Bayes often performs surprisingly well in practice and can be a good baseline model for classification tasks.\n",
        "\n",
        "#### For example\n",
        "Naive Bayes can be used for sentiment analysis of customer reviews, where it predicts whether a review is positive or negative based on the words used. It calculates the likelihood of a review belonging to a certain sentiment class based on the occurrence of specific words and their frequencies in positive and negative training samples."
      ],
      "metadata": {
        "id": "AYL7Opb-lIIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive Bayes:\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Create the Naive Bayes model\n",
        "naive_bayes_model = GaussianNB()\n",
        "\n",
        "# Train the model\n",
        "naive_bayes_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = naive_bayes_model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "gFDF4zKJkqiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Convolutional Neural Networks (CNN)\n",
        "Convolutional Neural Networks (CNN) are deep learning models designed specifically for image processing tasks. They consist of multiple layers, including convolutional layers that extract spatial features from input images. CNNs use a combination of convolutional, pooling, and fully connected layers to learn hierarchical representations of visual data. They are highly effective in tasks such as image classification, object detection, and image segmentation. CNNs are widely used in computer vision applications due to their ability to automatically learn relevant features from raw image data, leading to improved accuracy and efficiency in image analysis tasks.\n",
        "\n",
        "#### For example\n",
        "A CNN can also be used for facial recognition, where it can identify and recognize faces in images or videos. This can be applied in various applications such as surveillance systems, access control systems, or social media platforms for tagging people in photos."
      ],
      "metadata": {
        "id": "8lX7xZmXlJBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convolutional Neural Networks (CNN):\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Create the CNN model\n",
        "cnn_model = Sequential()\n",
        "cnn_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(image_width, image_height, channels)))\n",
        "cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "cnn_model.add(Flatten())\n",
        "cnn_model.add(Dense(128, activation='relu'))\n",
        "cnn_model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "cnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "cnn_model.fit(X_train, y_train, epochs=10)\n",
        "\n",
        "# Make predictions\n",
        "predictions = cnn_model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "Ad2dy3Rvk3aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Recurrent Neural Networks (RNN):\n",
        "Recurrent Neural Networks (RNNs) are a type of deep learning model that is designed to process sequential data, such as time series or natural language. Unlike feedforward neural networks, RNNs have recurrent connections that allow them to retain information from previous steps, making them suitable for tasks involving temporal dependencies. RNNs are widely used in applications such as speech recognition, machine translation, sentiment analysis, and time series forecasting. They excel in capturing long-term dependencies and patterns in sequential data, making them powerful tools for modeling and generating sequences.\n",
        "\n",
        "#### For example\n",
        "RNNs can be used for tasks like text generation, where the model learns from a given sequence of words and generates new text that follows a similar pattern. For example, they can be used to create chatbots that generate human-like responses or to generate song lyrics based on existing song lyrics."
      ],
      "metadata": {
        "id": "tOW_13a7lJ2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Recurrent Neural Networks (RNN):\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Create the RNN model\n",
        "rnn_model = Sequential()\n",
        "rnn_model.add(LSTM(128, input_shape=(timesteps, features)))\n",
        "rnn_model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "rnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "rnn_model.fit(X_train, y_train, epochs=10)\n",
        "\n",
        "# Make predictions\n",
        "predictions = rnn_model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "xo9LUmhXk8eT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### XGBoost\n",
        "XGBoost (eXtreme Gradient Boosting) is a popular machine learning algorithm that uses an ensemble of decision trees to make predictions. It is known for its high performance and efficiency in handling large datasets. XGBoost combines the strengths of gradient boosting and regularization techniques to improve model accuracy and reduce overfitting. It is widely used in various domains, including data analytics competitions and real-world applications, for tasks such as classification, regression, and ranking. XGBoost is especially effective when there are complex relationships and interactions among the features in the dataset.\n",
        "\n",
        "#### For example\n",
        "XGBoost can be applied in credit risk assessment, where the goal is to predict the likelihood of a borrower defaulting on a loan. By analyzing various features such as credit history, income, and loan amount, XGBoost can generate a predictive model to assess the creditworthiness of individuals and make informed lending decisions."
      ],
      "metadata": {
        "id": "G22mXiEGlKpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoost:\n",
        "import xgboost as xgb\n",
        "\n",
        "# Create the XGBoost model\n",
        "xgboost_model = xgb.XGBClassifier()\n",
        "\n",
        "# Train the model\n",
        "xgboost_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = xgboost_model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "_9LtjiIWlAqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation"
      ],
      "metadata": {
        "id": "neCVWwwTYY-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification metrics:\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "# Calculate precision\n",
        "precision = precision_score(y_true, y_pred)\n",
        "\n",
        "# Calculate recall\n",
        "recall = recall_score(y_true, y_pred)\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(y_true, y_pred)\n"
      ],
      "metadata": {
        "id": "s1FojKmsYa0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Regression metrics:\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Calculate mean squared error (MSE)\n",
        "mse = mean_squared_error(y_true, y_pred)\n",
        "\n",
        "# Calculate root mean squared error (RMSE)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# Calculate R-squared\n",
        "r2 = r2_score(y_true, y_pred)\n"
      ],
      "metadata": {
        "id": "nqA0CX7lAVC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-validation example:\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Perform k-fold cross-validation with a classifier\n",
        "scores = cross_val_score(classifier, X, y, cv=5)  # 5-fold cross-validation\n",
        "mean_score = scores.mean()\n"
      ],
      "metadata": {
        "id": "BGjnwxU2AYrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-test split example:\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model on the training set\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "score = model.score(X_test, y_test)\n"
      ],
      "metadata": {
        "id": "r7N-mn-dAbda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results and Analysis"
      ],
      "metadata": {
        "id": "5WSQu24YYbNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model and make predictions\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Get feature importance\n",
        "feature_importance = model.feature_importance_\n",
        "\n",
        "# Perform statistical tests\n",
        "# Example: t-test to compare two groups\n",
        "group1 = data[data['group'] == 'Group 1']\n",
        "group2 = data[data['group'] == 'Group 2']\n",
        "t_statistic, p_value = ttest_ind(group1['value'], group2['value'])\n",
        "\n",
        "# Print the results and analysis\n",
        "print(\"Results:\")\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "print(\"Feature Importance:\")\n",
        "for feature, importance in zip(features, feature_importance):\n",
        "    print(\"- {}: {:.2f}\".format(feature, importance))\n",
        "print(\"Statistical Test:\")\n",
        "print(\"t-statistic: {:.2f}\".format(t_statistic))\n",
        "print(\"p-value: {:.2f}\".format(p_value))\n",
        "\n",
        "# Additional analysis or visualizations can be performed here\n",
        "\n",
        "# Summarize the insights and conclusions\n",
        "print(\"\\nAnalysis:\")\n",
        "print(\"The model achieved an accuracy of {:.2f}% on the test set.\".format(accuracy * 100))\n",
        "print(\"The most important features for the model were:\")\n",
        "for feature, importance in zip(features, feature_importance):\n",
        "    print(\"- {}: {:.2f}\".format(feature, importance))\n",
        "print(\"The statistical test results suggest a significant difference between Group 1 and Group 2 in terms of the 'value' variable.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "9c_8PT91YdCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discussion and Conclusion"
      ],
      "metadata": {
        "id": "IJlO76c8Ydbi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Questions\n",
        "* How do the performance metrics of different models compare?\n",
        "* Which features were found to be the most important in predicting the target variable?\n",
        "* Are there any unexpected or interesting patterns or insights discovered from the analysis?\n",
        "* Were there any challenges or limitations that impacted the model performance?\n",
        "* How generalizable are the models to unseen data? Were cross-validation or train-test splits effective in assessing the model's performance?\n",
        "* What are the potential implications or applications of the findings?\n",
        "* Are there any ethical considerations or potential biases that need to be addressed?\n",
        "* How can the project be improved or expanded in the future?\n",
        "* Are there any additional experiments or analyses that could further validate or enhance the results?\n",
        "* What are the next steps or recommendations based on the findings of the project?"
      ],
      "metadata": {
        "id": "kNJsERvlBBba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EXAMPLE\n",
        "# Print the discussion and conclusion\n",
        "print(\"Discussion and Conclusion:\")\n",
        "\n",
        "# Discuss the findings\n",
        "print(\"The results of our analysis indicate that the machine learning models performed well in predicting the target variable.\")\n",
        "print(\"We observed high accuracy scores and significant feature importance, suggesting that the selected features were informative in the prediction task.\")\n",
        "\n",
        "# Discuss the limitations\n",
        "print(\"However, it is important to acknowledge some limitations of our project. Firstly, the dataset used for training and testing was relatively small, which could limit the generalization of the models.\")\n",
        "print(\"Additionally, there may be other relevant features not included in our dataset that could improve the model performance.\")\n",
        "\n",
        "# Discuss potential future directions\n",
        "print(\"To further enhance the project, future directions could include collecting a larger and more diverse dataset to improve model generalization.\")\n",
        "print(\"Moreover, exploring advanced techniques such as ensemble learning or deep learning architectures could be beneficial in capturing more complex patterns in the data.\")\n",
        "\n",
        "# Reflect on the overall success and challenges\n",
        "print(\"Overall, our project was successful in achieving our initial objectives and producing accurate predictions.\")\n",
        "print(\"However, we encountered challenges in data preprocessing and feature engineering, which required significant effort and domain expertise.\")\n",
        "\n",
        "# Conclude the discussion\n",
        "print(\"In conclusion, our project demonstrates the effectiveness of machine learning models in predicting the target variable.\")\n",
        "print(\"Further improvements and research are warranted to address the limitations and explore new avenues for enhancing the model's performance.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "lBrWLbEUYfTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References"
      ],
      "metadata": {
        "id": "0E-nIzGCYfqC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The author would like to express gratitude to the following individuals whose exceptional work and publications served as valuable sources for the creation of this tutorial:"
      ],
      "metadata": {
        "id": "pBOgIp8MYkMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   AlexNet: Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). \"ImageNet Classification with Deep Convolutional Neural Networks.\" [Link to Paper](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)\n",
        "*   LSTM: Hochreiter, S., & Schmidhuber, J. (1997). \"Long Short-Term Memory.\" [Link to Paper](https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory)\n",
        "\n",
        "*   GAN: Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., ... & Bengio, Y. (2014). \"Generative Adversarial Networks.\" [Link to Paper](https://arxiv.org/pdf/1406.2661.pdf)\n",
        "*   Word2Vec: Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). \"Distributed Representations of Words and Phrases and their Compositionality.\" [Link to Paper](https://arxiv.org/pdf/1310.4546.pdf)\n",
        "\n",
        "*   ResNet: He, K., Zhang, X., Ren, S., & Sun, J. (2016). \"Deep Residual Learning for Image Recognition.\" [Link to Paper](https://arxiv.org/pdf/1512.03385.pdf)\n",
        "*   Transformer: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). \"Attention is All You Need.\" [Link to Paper](https://arxiv.org/pdf/1706.03762.pdf)\n",
        "\n",
        "*   Mask R-CNN: He, K., Gkioxari, G., Doll√°r, P., & Girshick, R. (2017). \"Mask R-CNN.\" [Link to Paper](https://arxiv.org/pdf/1703.06870.pdf)\n",
        "*   DeepFace: Taigman, Y., Yang, M., Ranzato, M., & Wolf, L. (2014). \"DeepFace: Closing the Gap to Human-Level Performance in Face Verification.\" [Link to Paper](https://www.cs.toronto.edu/~ranzato/publications/taigman_cvpr14.pdf)\n",
        "\n",
        "*   U-Net: Ronneberger, O., Fischer, P., & Brox, T. (2015). \"U-Net: Convolutional Networks for Biomedical Image Segmentation.\" [Link to Paper](https://arxiv.org/pdf/1505.04597.pdf)\n",
        "*   AlphaGo: Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., ... & Hassabis, D. (2016). \"Mastering the Game of Go with Deep Neural Networks and Tree Search.\" [Link to Paper](https://www.nature.com/articles/nature16961)\n"
      ],
      "metadata": {
        "id": "9taNkgOVYOsJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Repo\n",
        "Please refer to the project repository for more details and updates:\n",
        "[Project Repo](https://github.com/natnew/100-Days-of-Artificial-Intelligence/tree/main)\n",
        "\n",
        "## Usage and Attribution\n",
        "If you find this material useful, we kindly request that you give appropriate credit and attribute the project repository in your work. You can include a reference to the project repo in your code comments, documentation, or acknowledgments section, like this:\n",
        "\n",
        "'''\n",
        "Code adapted from the \"100-Days-of-Artificial-Intelligence\" project repository by [100-Days-of-Artificial-Intelligence](https://github.com/natnew/100-Days-of-Artificial-Intelligence/tree/main)\n",
        "'''\n",
        "\n",
        "Thank you for your understanding and support!"
      ],
      "metadata": {
        "id": "zMhzbDwqhFF5"
      }
    }
  ]
}